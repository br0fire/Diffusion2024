{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bb8ead",
   "metadata": {
    "id": "c3bb8ead"
   },
   "source": [
    "# CV Week: Итоговое задание\n",
    "\n",
    "На лекции и семинаре мы разбирали как дистиллировать многошаговую диффузионную модель в малошагового студента, и тем самым будет работать на порядок быстрее учителя.\n",
    "\n",
    "Один из подходов, который мы разбирали *Consistency Distillation*. В этом задании, мы закрепим материал, который был на лекции и семинаре и реализуем этот фреймворк, затрагивая различные нюансы.\n",
    "\n",
    "В этом задании мы будем дистиллировать модель *Stable Diffusion 1.5 (SD1.5)* для генерации картинок по текстовому описанию.\n",
    "\n",
    "Вам предстоит выполнить 8 небольших заданий, которые приведут нас к неплохой модели для генерации картинок за 4 шага, работая в органиченных условиях колаба."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585e5265",
   "metadata": {
    "id": "585e5265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: diffusers==0.30.3 in /home/jupyter/.local/lib/python3.10/site-packages (0.30.3)\n",
      "Requirement already satisfied: peft==0.8.2 in /home/jupyter/.local/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: huggingface_hub==0.23.4 in /home/jupyter/.local/lib/python3.10/site-packages (0.23.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers==0.30.3) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.3) (3.12.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.3) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.3) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.3) (2.27.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from diffusers==0.30.3) (0.4.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.30.3) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.8.2) (23.1)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from peft==0.8.2) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.8.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.8.2) (2.0.1+cu118)\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (from peft==0.8.2) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.8.2) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/jupyter/.local/lib/python3.10/site-packages (from peft==0.8.2) (1.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.23.4) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.23.4) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.8.2) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.8.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.8.2) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.8.2) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.8.2) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.8.2) (16.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.30.3) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.30.3) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.30.3) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.30.3) (3.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers->peft==0.8.2) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.8.2) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# torch 2.4.1+cu124\n",
    "%pip install diffusers==0.30.3, peft==0.8.2 huggingface_hub==0.23.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4aafa",
   "metadata": {
    "id": "67e4aafa"
   },
   "source": [
    "## Теормин\n",
    "\n",
    "---\n",
    "##### Диффузионные модели\n",
    "\n",
    "Задан прямой диффузионный процесс, который переводит чистые картинки в шум с помощью распределения $q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)$\n",
    "\n",
    "Таким образом, мы можем получаться зашумленные картинки по следующей формуле: $\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon$, где $\\epsilon{\\sim} {N}(0, I)$ **(1)**\n",
    "\n",
    "$\\alpha_t, \\sigma_t$ задают процесс зашумления. Здесь мы будем иметь дело с *variance preserving (VP)* процессом, т. е., $\\alpha^2_t = 1 - \\sigma^2_t$.\n",
    "\n",
    "Диффузионная модель (ДМ) пытается решить обратную задачу: из шума порождать новые картинки.\n",
    "Важно, что диффузионный процесс можно описать следующим обыкновенным дифференциальным уравнением (ОДУ):\n",
    "\n",
    " $dx = \\left[ f(\\mathbf{x}, t) - \\frac{1}{2} \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}) \\right] dt$, **(2)**\n",
    "\n",
    "где $f(\\mathbf{x}, t)$ известен из заданного процесса зашумления, а $\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)$ (*скор функцию*) оцениваем с помощью нейросети: $s_\\theta(\\mathbf{x}_t, t) \\approx \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)$. Таким образом, имея оценку на $\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x})$, мы можем решить это ОДУ, стартуя со случайного шума, и получить картинку.\n",
    "\n",
    "\n",
    "**SD1.5** использует *$\\epsilon$-параметризацию*, т.е., UNet пытается предсказать шум, который мы добавили на картинку по формуле **(1)**. Оценку скор функции можно получить, пользуясь результатом, вытекающим из формулы Твидди: $s_\\theta(\\mathbf{x}_t, t) = - \\frac{\\epsilon_\\theta(\\mathbf{x}_t, t)} { \\sigma_t}$\n",
    "\n",
    "Чтобы решить ОДУ **(2)**, нам нужно воспользоваться каким-то численным методом (солвером). В этом задании мы будем работать с не самым эффектным, но самым популярным солвером: **DDIM**, который является адаптированным методом Эйлера под диффузионный ОДУ.\n",
    "\n",
    "Для VP процесса переход с помощью DDIM с шага $t$ на $s$ можно сделать следующим образом:\n",
    "\n",
    "$\n",
    "x_s = DDIM(\\mathbf{x}_t, t, s) = \\alpha_s \\cdot \\left(\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t} \\right) + \\sigma_s \\epsilon_\\theta\n",
    "$\n",
    "\n",
    "Этот переход можно интерпретировать так: получаем оценку на чистую картинку $\\mathbf{x}_0$ на шаге $t$, используя $\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t}$, а потом снова зашумляем эту оценку на шаг $s$ по формуле **(1)**, но только используем не случайный шум, а шум предсказанный моделью $\\epsilon_\\theta$.\n",
    "\n",
    "*Используя DDIM для SD1.5, можем получать хорошие картинки за 50 шагов.*\n",
    "\n",
    "**SD1.5** - латентная ДМ, т.е. модель работает не в пиксельном пространстве, а в латентном пространстве **VAE**. Таким образом SD1.5 состоит из следующих компонент:\n",
    "\n",
    "1) **VAE** - переводит $3{\\times}512{\\times}512$ картинки в латенты $4{\\times}64{\\times}64$ и может декодировать их обратно в картинки.\n",
    "2) **Текстовый энкодер** - извлекает текстовые признаки из промпта. Эти признаки будут подаваться в диффузионную модель, чтобы дать модели информацию, что именно хотим сгенерировать\n",
    "3) **Диффузионная модель** - UNet, работающий на \"латентных картинках\" $4{\\times}64{\\times}64$.\n",
    "\n",
    "---\n",
    "#### Консистенси модели\n",
    "\n",
    "##### Общая идея\n",
    "\n",
    "Главная цель дистилляции диффузии - уменьшить количество шагов ДМ, при этом сохранив высокое качество картинок.\n",
    "\n",
    "**Консистенси модели (Consistency Models | CM)** - класс моделей, где мы хотим выучить \"консистенси функцию\" $f_\\theta(\\mathbf{x}_t)$ - с любой точки $\\mathbf{x}_{t}$ траектории диффузионного ОДУ **(2)** сразу предсказывать $\\mathbf{x}_{0}$ (чистые данные) за один шаг. \n",
    "Если мы идеально выучим консистенси функцию, то сможем шагать из чистого шума сразу в картинку, что супер эффективно в отличии от генерации ДМ. \n",
    "\n",
    "Отметим, что консистенси модель можно учить как независимую генеративную модель, без предобученной ДМ, и в *задании 3* вам предстоит подумать, как это можно сделать.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "\n",
    "<img src=\"https://i.postimg.cc/prgMVw6C/cd-idea.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "**Консистенси дистилляция (Consistency Distillation | CD)** - подход, когда для обучения CM, мы используем предобученную ДМ. ДМ нам дает качественную инициализацию модели и уже обученную скор функцию, что сильно упрощает сходимость консистенси моделей. \n",
    "\n",
    "\n",
    "### Обучение CM\n",
    "\n",
    "<div>\n",
    "<img src=\"https://i.postimg.cc/2jnsnKMs/cd-training.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Главная принцип обучения консистенси моделей заключается в попытке удовлетворить *self-consistency* св-ву: выход CM на двух соседних точках траектории $\\mathbf{x}_{t}$ и $\\mathbf{x}_{t-1}$ должен совпадать по какой-то мере близости, например L2 расстояние: $\\lVert f_\\theta(\\mathbf{x}_{t-1}) - f_\\theta(\\mathbf{x}_{t}) \\rVert^2_2$.\n",
    "\n",
    "Заметим, что self-consistency св-во удовлетворить очень просто без какого-либо обучения, взяв, например $f_\\theta(\\mathbf{x}_{t}) \\equiv 0$.\n",
    "\n",
    "Поэтому, чтобы избежать вырожденных решений, нам необходимо выставить граничное условие (boundary condition), которое будет требовать, чтобы в самой левой точке траектории около 0, модель предсказывала картинку, которую получает на вход: $f_\\theta(\\mathbf{x}_{\\epsilon}) = \\mathbf{x}_{\\epsilon}$.\n",
    "\n",
    "**Практическое замечание:** Для обеих точек траектории мы применяем одну и ту же модель $f_\\theta(\\cdot)$. Но выход модели на шаге ${t-1}$ является \"таргетом\" для выхода модели на шаге $t$ и поэтому выполнение модели для шага $t-1$ выполняется в *torch.no_grad* режиме.\n",
    "\n",
    "**Как получаться две соседние точки на траектории ОДУ?**\n",
    "\n",
    "Берем случайную картинку $\\mathbf{x}_0$ из датасета.\n",
    "\n",
    "Точку $\\mathbf{x}_t$ получаем с помощью прямого процесса зашумления: $\\mathbf{x}_t = q(\\mathbf{x}_t | \\mathbf{x}_0)$\n",
    "\n",
    "Чтобы получить соседнюю точку $\\mathbf{x}_{t-1}$, нам нужно сделать шаг по траектории ОДУ, используя, например, DDIM солвер. \n",
    "\n",
    "В консистенси дистилляции, мы делаем шаг предобученной ДМ: $\\mathbf{x}_{t-1} = DDIM(\\epsilon_\\theta(\\mathbf{x}_t, t), \\mathbf{x}_t, t, t-1)$\n",
    "\n",
    "**Важно:** на практике мы можем брать не соседние шаги $t$ и $t-1$, а с некоторым интервалом, например 20 шагов. \n",
    "Размер интервала влияет на bias/variance trade-off в консистенси обучении: больше интервал между шагами - больше смещение, но меньше дисперсия, и наоборот. \n",
    "Для простоты в этом задании мы зафиксируем интервал - 20 шагов, но во многих работах размер интервала динамически меняют по ходу обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18dffaf",
   "metadata": {
    "id": "c18dffaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-12-18 16:20:32.916749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, LCMScheduler, UNet2DConditionModel, DDIMScheduler\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669d0a3a",
   "metadata": {
    "id": "669d0a3a"
   },
   "outputs": [],
   "source": [
    "#---------------------\n",
    "# Visualization utils\n",
    "#---------------------\n",
    "\n",
    "def visualize_images(images):\n",
    "    assert len(images) == 4\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=-0.01, hspace=-0.01)\n",
    "\n",
    "\n",
    "#--------------\n",
    "# Tensor utils\n",
    "#--------------\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "#---------------\n",
    "# Dataset utils\n",
    "#---------------\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, subset_name=\"train2014_5k\", transform=None, max_cnt=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.extensions = (\n",
    "            \".jpg\",\n",
    "            \".jpeg\",\n",
    "            \".png\",\n",
    "            \".ppm\",\n",
    "            \".bmp\",\n",
    "            \".pgm\",\n",
    "            \".tif\",\n",
    "            \".tiff\",\n",
    "            \".webp\",\n",
    "        )\n",
    "        sample_dir = os.path.join(root_dir, subset_name)\n",
    "\n",
    "        # Collect sample paths\n",
    "        self.samples = sorted(\n",
    "            [\n",
    "                os.path.join(sample_dir, fname)\n",
    "                for fname in os.listdir(sample_dir)\n",
    "                if fname[-4:] in self.extensions\n",
    "            ],\n",
    "            key=lambda x: x.split(\"/\")[-1].split(\".\")[0],\n",
    "        )\n",
    "        self.samples = (\n",
    "            self.samples if max_cnt is None else self.samples[:max_cnt]\n",
    "        )  # restrict num samples\n",
    "\n",
    "        # Collect captions\n",
    "        self.captions = {}\n",
    "        with open(\n",
    "            os.path.join(root_dir, f\"{subset_name}.csv\"), newline=\"\\n\"\n",
    "        ) as csvfile:\n",
    "            spamreader = csv.reader(csvfile, delimiter=\",\")\n",
    "            for i, row in enumerate(spamreader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                self.captions[row[1]] = row[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample_path = self.samples[idx]\n",
    "        sample = Image.open(sample_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return {\n",
    "            \"image\": sample,\n",
    "            \"text\": self.captions[os.path.basename(sample_path)],\n",
    "            \"idxs\": idx, }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62860269",
   "metadata": {
    "id": "62860269"
   },
   "source": [
    "# Модель учителя (SD1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce973eb",
   "metadata": {
    "id": "8ce973eb"
   },
   "source": [
    "## Задание №1\n",
    "\n",
    "Давайте для начала загрузим модель [StableDiffusion 1.5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) и сгенерируем ей картинки за 50 шагов.\n",
    "\n",
    "**Важно:** для экономии памяти, загружаем все компоненты модели в FP16. Не забываем положить модель на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46016d",
   "metadata": {
    "id": "4e46016d"
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline()\n",
    "\n",
    "# Проверяем, что все компоненты модели в FP16 и на cuda\n",
    "assert pipe.unet.dtype == torch.float16 and pipe.unet.device.type == 'cuda'\n",
    "assert pipe.vae.dtype == torch.float16 and pipe.vae.device.type == 'cuda'\n",
    "assert pipe.text_encoder.dtype == torch.float16 and pipe.text_encoder.device.type == 'cuda'\n",
    "\n",
    "# Заменяем дефолтный сэмплер на DDIM\n",
    "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "pipe.scheduler.timesteps = pipe.scheduler.timesteps.cuda()\n",
    "pipe.scheduler.alphas_cumprod = pipe.scheduler.alphas_cumprod.cuda()\n",
    "\n",
    "# Отдельно извлечем модель учителя, которую потом будем дистиллировать\n",
    "teacher_unet = pipe.unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb66c9",
   "metadata": {
    "id": "20cb66c9"
   },
   "source": [
    "Теперь сгенерируем картинки за 50 шагов. Вам нужно написать вызов pipe и передать в него промпт, число шагов генерации, генератор случайных чисел, гайденс скейл и указать, чтобы сгенерировалось 4 картинки на промпт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb382b",
   "metadata": {
    "id": "d5bb382b"
   },
   "outputs": [],
   "source": [
    "prompt = \"A sad puppy with large eyes\"\n",
    "guidance_scale = 7.5\n",
    "generator = torch.Generator('cuda').manual_seed(1)\n",
    "\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8a547",
   "metadata": {
    "id": "09c8a547"
   },
   "source": [
    "Давайте посмотрим, что выдаст модель за 4 шага.\n",
    "Все то же самое, что и выше, просто поменяем число шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e66279",
   "metadata": {
    "id": "06e66279"
   },
   "outputs": [],
   "source": [
    "generator = torch.Generator('cuda').manual_seed(1)\n",
    "\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb5003",
   "metadata": {
    "id": "b0cb5003"
   },
   "source": [
    "На 4 шагах картинки получаются размазанными. Давайте постараемся починить их."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82774c59",
   "metadata": {
    "id": "82774c59"
   },
   "source": [
    "##  Создаем датасет\n",
    "\n",
    "Чтобы ДЗ было легко выполнимым на colab, мы будем учить консистенси модели на небольшой обучающей выборке из 5000 пар текст-картинка из COCO датасета.\n",
    "Интересное свойство консистенси моделей - они могут сходиться до адекватного качества за несколько сотен шагов. Качество все еще будет не идеальным, но фазовый переход уже должен быть заметен.\n",
    "\n",
    "Данные можно загрузить с помощью команд в ячейке ниже. В локальной текущей директории ./ должны появиться:\n",
    "* Папка train2014_5k с 5000 картинками\n",
    "* Файл train2014_5k.csv с 5000 промптами\n",
    "\n",
    "Данные парсятся корректным образом в уже реализованном классе COCODataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431a4e8",
   "metadata": {
    "id": "2431a4e8"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.yandexcloud.net/yandex-research/train2014_5k.tar.gz\n",
    "!tar -xzf train2014_5k.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc010814",
   "metadata": {
    "id": "bc010814"
   },
   "source": [
    "**Замечание:** для более быстрого дебаггинга можете взять, например, 2500 картинок и прогнать на всей выборке только в самом конце. 2500 картинок должно быть достаточно для понимания корректно ли реализованы функции.\n",
    "Совсем для первичного дебаггинга можно взять еще меньше картинок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba81fb",
   "metadata": {
    "id": "e1ba81fb"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: 2 * x - 1,\n",
    "    ]\n",
    ")\n",
    "dataset = COCODataset(\".\",\n",
    "    subset_name=\"train2014_5k\",\n",
    "    transform=transform,\n",
    "#     max_cnt=2500\n",
    ")\n",
    "assert len(dataset) == 5000 # 2500\n",
    "\n",
    "batch_size = 8 # Рекоммендуемы размер батча на Colab\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad222c2",
   "metadata": {
    "id": "cad222c2"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_batch(batch, pipe):\n",
    "    \"\"\"\n",
    "    Предобработка батча картинок и текстовых промптов.\n",
    "    Маппим картинки в латентное пространство VAE.\n",
    "    Извлекаем эмбеды промптов с помощью текстового энкодера.\n",
    "    \n",
    "    Params:\n",
    "\n",
    "    Return:\n",
    "        latents: torch.Tensor([B, 4, 64, 64], dtype=torch.float16)\n",
    "        prompt_embeds: torch.Tensor([B, 77, D], dtype=torch.float16)\n",
    "    \"\"\"\n",
    "\n",
    "    # Токенизируем промпты\n",
    "    text_inputs = pipe.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=\"max_length\",\n",
    "        max_length=pipe.tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Извлекаем эмбеды промптов с помощью текстового энкодера\n",
    "    prompt_embeds = pipe.text_encoder(text_inputs.input_ids.cuda())[0]\n",
    "\n",
    "    # Переводим картинки в латентное пространство VAE\n",
    "    image = batch['image'].to(\"cuda\", dtype=torch.float16)\n",
    "    latents = pipe.vae.encode(image).latent_dist.sample()\n",
    "    latents = latents * pipe.vae.config.scaling_factor\n",
    "    return latents, prompt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a00a9",
   "metadata": {
    "id": "062a00a9"
   },
   "source": [
    "### Подготовка моделей и оптимизатора\n",
    "\n",
    "Для начала создаем обучаемую модель: UNet инициализируемый весами SD1.5.\n",
    "Вам нужно воспользоваться классом UNet2DConditionModel и загрузить отдельно только UNet модель из SD1.5.\n",
    "\n",
    "Отметим, что эта модель у нас будет храниться в полной точности FP32, потому что обучение параметров в FP16 может приводить к нестабильностям и низкому качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e73f1a",
   "metadata": {
    "id": "e2e73f1a"
   },
   "outputs": [],
   "source": [
    "unet = <YOUR CODE HERE>\n",
    "\n",
    "assert unet.dtype == torch.float32\n",
    "assert unet.training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f56cb",
   "metadata": {
    "id": "cd4f56cb"
   },
   "source": [
    "Для экономии памяти во время обучения будем учить не параметры самой модели, а добавим в нее обучаемые LoRA адаптеры с малым числом параметров.\n",
    "\n",
    "LoRA представляет собой маленькую добавку к весам модели, где на одну матрицу весов $W \\in \\mathbb{R}^{m{\\times}n} $ обучаются две низкоранговые матрицы $W_A \\in \\mathbb{R}^{k{\\times}n}$ и $W_B \\in \\mathbb{R}^{k{\\times}m}$, где $k$ - ранг матрицы сильно меньше $m$ и $n$.\n",
    "\n",
    "Тем самым, новая обученная матрица весов может быть представлена как $\\hat{W} = W + \\Delta W = W + W^T_B W_A$.  \n",
    "Во время инференса $\\Delta W$ можно вмержить в $W$ и получить итоговую модель. \n",
    "Также частая практика оставлять адаптеры как есть, чтобы была возможность для одной базовой модели учить несколько адаптеров под разные задачи и переключаться между ними по необходимости.\n",
    "\n",
    "Если не мержить адаптеры, то вычисления для линейного слоя происходят как на картинке ниже.\n",
    "\n",
    "<img src=https://miro.medium.com/v2/resize:fit:680/format:webp/0*2meitaJ7pdUusbb5.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bb504",
   "metadata": {
    "id": "020bb504"
   },
   "outputs": [],
   "source": [
    "# Указываем к каким слоям модели мы будет добавлять адаптеры.\n",
    "lora_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\", \"proj_in\", \"proj_out\",\n",
    "    \"ff.net.0.proj\", \"ff.net.2\", \"conv1\", \"conv2\", \"conv_shortcut\",\n",
    "    \"downsamplers.0.conv\", \"upsamplers.0.conv\", \"time_emb_proj\"\n",
    "]\n",
    "lora_config = LoraConfig(\n",
    "    r=64, # задает ранг у матриц A и B в LoRA.\n",
    "    target_modules=lora_modules\n",
    ")\n",
    "\n",
    "# Создаем обертку исходной UNet модели с LoRA адаптерами, используя библиотеку PEFT\n",
    "cm_unet = get_peft_model(unet, lora_config, adapter_name=\"ct\")\n",
    "\n",
    "# Включаем gradient checkpointing - важная техника для экономии памяти во время обучения\n",
    "cm_unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Создаем оптимизатор\n",
    "optimizer = torch.optim.AdamW(cm_unet.parameters(), lr=1e-4)\n",
    "\n",
    "# Задаем лосс функцию для CM обжектива. В базовом варианте разумно взять L2\n",
    "# По умолчанию, она уже выдает усредненное значение по всем размерностям\n",
    "mse_loss = torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf9615",
   "metadata": {
    "id": "52bf9615"
   },
   "source": [
    "## Задание №2 (0.5 балла, сдается в контесте)\n",
    "\n",
    "####  Реализация шага DDIM\n",
    "\n",
    "Шаг с помощью DDIM с $\\mathbf{x}_t$ на $\\mathbf{x}_s$ можно сделать следующим образом:\n",
    "\n",
    "$\n",
    "\\mathbf{x}_s = DDIM(\\epsilon_\\theta, \\mathbf{x}_t, t, s) = \\alpha_s \\cdot \\left(\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t} \\right) + \\sigma_s \\epsilon_\\theta\n",
    "$\n",
    "\n",
    "Вам нужно реализовать эту формулу в уже готовом шаблоне ниже.\n",
    "Чтобы корректно выполнить задание, вам нужно задать $\\alpha_t$ и $\\sigma_t$ имея *DDIMScheduler*.\n",
    "**Обратите внимание на аттрибут *scheduler.alphas_cumprod***, который задает $\\bar\\alpha_{t} = \\prod^t_{i=1} (1-\\beta_i)$ в классической DDPM формулировке: [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743504b",
   "metadata": {
    "id": "d743504b"
   },
   "outputs": [],
   "source": [
    "def ddim_solver_step(model_output, x_t, t, s, scheduler):\n",
    "    \"\"\"\n",
    "    Шаг DDIM солвера для VP процесса зашумления и eps-prediction модели\n",
    "    params:\n",
    "        model_output: torch.Tensor[B, 4, 64, 64] - предсказание модели - шум eps\n",
    "        x_t: torch.Tensor[B, 4, 64, 64] - сэмплы на шаге t\n",
    "        t: torch.Tensor[B] - номер текущего шага\n",
    "        s: torch.Tensor[B] - номер следующего шага\n",
    "        scheduler: DDIMScheduler - расписание диффузионного процесса, чтобы получить alpha и sigma\n",
    "    \"\"\"\n",
    "    alphas = <YOUR CODE HERE>\n",
    "    sigmas = <YOUR CODE HERE>\n",
    "\n",
    "    sigmas_s = extract_into_tensor(sigmas, s, x_t.shape)\n",
    "    alphas_s = extract_into_tensor(alphas, s, x_t.shape)\n",
    "\n",
    "    sigmas_t = extract_into_tensor(sigmas, t, x_t.shape)\n",
    "    alphas_t = extract_into_tensor(alphas, t, x_t.shape)\n",
    "\n",
    "    # Выставляем крайние значения alpha и sigma, чтобы выполнялись граничные условия\n",
    "    alphas_s[s == 0] = 1.0\n",
    "    sigmas_s[s == 0] = 0.0\n",
    "\n",
    "    alphas_t[t == 0] = 1.0\n",
    "    sigmas_t[t == 0] = 0.0\n",
    "\n",
    "    x_0 = <YOUR CODE HERE> # x0 оценка на шаге t\n",
    "    x_s = <YOUR CODE HERE> # Переход на шаг s\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4013fc3",
   "metadata": {
    "id": "c4013fc3"
   },
   "source": [
    "####  Реализация процесса зашумления (q sample)\n",
    "\n",
    "Аналогично, нам нужен процесс зашумления $q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)$\n",
    "\n",
    "$\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon$, где $\\epsilon{\\sim} {N}(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb169c",
   "metadata": {
    "id": "9fdb169c"
   },
   "outputs": [],
   "source": [
    "def q_sample(x, t, scheduler, noise=None):\n",
    "    alphas = <YOUR CODE HERE>\n",
    "    sigmas = <YOUR CODE HERE>\n",
    "\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "    sigmas_t = extract_into_tensor(sigmas, t, x.shape)\n",
    "    alphas_t = extract_into_tensor(alphas, t, x.shape)\n",
    "\n",
    "    x_t = <YOUR CODE HERE>\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5893c",
   "metadata": {
    "id": "21c5893c"
   },
   "source": [
    "# Consistency Training\n",
    "\n",
    "Обучение консистенси моделей без учителя называется Consistency Training (CT).\n",
    "В таком случае CM можно рассматривать как отдельный вид генеративных моделей.\n",
    "Давайте начнем именно с этого подхода и обучим нашу первую консистенси модель на базе SD1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5743b1",
   "metadata": {
    "id": "6d5743b1"
   },
   "source": [
    "## Задание №3 \n",
    "\n",
    "#### Задание №3.1 (0.5 балла, сдается в контесте)\n",
    "\n",
    "В консиcтенси дистилляции модель учителя используется для получения второй точки на траектории ODE.\n",
    "Можем ли мы попробовать оценить соседнюю точку аналитически?\n",
    "\n",
    "Вам предлагается вывести это самим, используя формулу DDIM шага выше и вспомнив, как мы оцениваем скор функции в denoising score matching-e:\n",
    "\n",
    "$\\epsilon_\\theta(x_t, t) = - \\sigma_t s_\\theta(x_t, t)$\n",
    "\n",
    "$s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t) = \\mathop{\\mathbb{E}}_{\\mathbf{x}\\sim p_{data}}\\left [ \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t | \\mathbf{x}) \\vert \\mathbf{x}_t \\right ] \\approx \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t \\vert \\mathbf{x})$\n",
    "\n",
    "---\n",
    "\n",
    "< YOUR DERIVATION HERE >\n",
    "    \n",
    "$x_s = ?$\n",
    "\n",
    "---\n",
    "    \n",
    "Если возникнут трудность, можно обратиться к оригинальной [статье](https://arxiv.org/pdf/2303.01469).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a321f6b",
   "metadata": {
    "id": "6a321f6b"
   },
   "source": [
    "Теперь реализуем то, что у вас получилось в функции ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58cba2",
   "metadata": {
    "id": "ff58cba2"
   },
   "outputs": [],
   "source": [
    "def get_xs_from_xt_naive(\n",
    "    x_0, x_t, t, s, # Не все эти аргументы могут быть вам нужны\n",
    "    scheduler,\n",
    "    noise=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Получение точки x_s в CT режиме, т.е., аналитически.\n",
    "    \"\"\"\n",
    "    x_s = <YOUR CODE HERE>\n",
    "    return x_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967d0f5",
   "metadata": {
    "id": "a967d0f5"
   },
   "source": [
    "#### Задание №3.2\n",
    "\n",
    "Ниже предстален шаблон функции, которая считает лосс для консистенси моделей. \n",
    "Вам нужно правильно заполнить пропуски, чтобы получилась корректная функция. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675acdd5",
   "metadata": {
    "id": "675acdd5"
   },
   "outputs": [],
   "source": [
    "def cm_loss_template(\n",
    "    latents, prompt_embeds, # батч латентов и текстовых эмбедов\n",
    "    unet, scheduler,\n",
    "\n",
    "    # Функции, которые будем постепенно менять из задания к заданию\n",
    "    loss_fn: callable,\n",
    "    get_boundary_timesteps: callable,\n",
    "    get_xs_from_xt: callable,\n",
    "\n",
    "    num_timesteps=1000,\n",
    "    step_size=20, # Указываем с каким интервалом берем шаги s и t.\n",
    "):\n",
    "    # Сэмплируем случайные шаги t для каждого элемента батча t ~ U[step_size-1, 999]\n",
    "    assert num_timesteps == 1000\n",
    "    num_intervals = num_timesteps // step_size\n",
    "\n",
    "    index = torch.randint(1, num_intervals, (len(latents),), device=latents.device).long() # [1, num_intervals]\n",
    "    t = step_size * index - 1\n",
    "    s = torch.clamp(t - step_size, min=0)\n",
    "    boundary_timesteps = get_boundary_timesteps(\n",
    "        s, num_timesteps=num_timesteps\n",
    "    )\n",
    "\n",
    "    # Сэмплируем x_t\n",
    "    noise = torch.randn_like(latents)\n",
    "    x_t = <YOUR CODE HERE>\n",
    "\n",
    "    # with <YOUR CODE HERE>: # для реализации mixed-precision обучения в задании №4\n",
    "    noise_pred = unet(x_t.float(), t,\n",
    "        encoder_hidden_states=prompt_embeds.float(),\n",
    "    ).sample\n",
    "\n",
    "    # Получаем оценку в граничной точке для x_t\n",
    "    boundary_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Получаем сэмпл x_s из x_t\n",
    "    x_s = get_xs_from_xt(\n",
    "        latents, x_t, t, s,\n",
    "        scheduler,\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        noise=noise,\n",
    "    )\n",
    "\n",
    "    # Предсказание \"таргет моделью\"\n",
    "    with torch.no_grad(), torch.amp.autocast(\"cuda\", torch.float16):\n",
    "        target_noise_pred = unet(x_s, s, encoder_hidden_states=prompt_embeds).sample\n",
    "\n",
    "    # Получаем оценку в граничной точке для x_s\n",
    "    boundary_target = <YOUR CODE HERE>\n",
    "\n",
    "    loss = loss_fn(boundary_pred, boundary_target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c0c60d",
   "metadata": {
    "id": "63c0c60d"
   },
   "outputs": [],
   "source": [
    "def get_zero_boundary_timesteps(t, **kwargs):\n",
    "    \"\"\"\n",
    "    Определяем шаги где будут срабатывать граничные условия.\n",
    "    Для классических СM это t=0.\n",
    "    \"\"\"\n",
    "    return torch.zeros_like(t)\n",
    "\n",
    "\n",
    "ct_loss = functools.partial(\n",
    "    cm_loss_template,\n",
    "\n",
    "    loss_fn=mse_loss,\n",
    "    get_boundary_timesteps=get_zero_boundary_timesteps,\n",
    "    get_xs_from_xt=get_xs_from_xt_naive\n",
    ")\n",
    "assert cm_unet.active_adapter == 'ct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c526cc4",
   "metadata": {
    "id": "5c526cc4"
   },
   "source": [
    "## Задание №4\n",
    "\n",
    "### Эффективное обучение\n",
    "Данное задание рассчитано на успешное выполнение на colab с бесплатной Tesla T4 c 15GB VRAM.\n",
    "Однако учить даже относительно небольшие T2I модели масштаба SD1.5 уже на коллабе в лоб проблематично.\n",
    "\n",
    "Для этого нам нужно применить ряд инженерных техник, чтобы уместиться в данный бюджет и учиться за разумное время.\n",
    "\n",
    "**Список техник**\n",
    "\n",
    "1) Включить gradient checkpointing для обучемой модели\n",
    "2) Добавить LoRA (Low Rank Adapters) адаптеры, чтобы учить не все веса, а только 10% добавочных весов\n",
    "3) Использовать gradient accumulation, чтобы делать итерацию обучения по бОльшему батчу, чем влезает по памяти\n",
    "4) Добавить mixed precision FP16/FP32 обучение модели для скорости. Обычно еще и память экономится, но в случае LoRA обучения + gradient checkpointing на память сильно влиять не должно, но зато станет быстрее.\n",
    "5) Мульти-GPU обучение - распределение вычислений по нескольким GPU.  \n",
    "\n",
    "1-2) Мы уже применили за вас выше\n",
    "\n",
    "3-4) Предстоит реализовать вам самим в соотвествующей секции ниже\n",
    "\n",
    "5 ) Недоступно, так как работаем на одной карточке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbf465",
   "metadata": {
    "id": "81dbf465"
   },
   "source": [
    "### Обучающий цикл\n",
    "\n",
    "Вам дан код обучения модель в полной точности (FP32) c батчом 8.\n",
    "К сожалению, на Tesla T4 мы не влезем по памяти.\n",
    "Поэтому в ячейке ниже вам нужно модифицировать цикл, чтобы он работал в mixed precision FP16 и добавить gradient accumulation.\n",
    "\n",
    "Про реализацию mixed-precision в pytorch можно перейти по ссылке: [Mixed-precision обучение](https://pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training)\n",
    "\n",
    "**Обратите внимание**: вам еще нужно добавить одну строчку кода в *cm_loss_template* в соответствующем плейсхолдере.\n",
    "\n",
    "**Замечание:** В начале обучения значения лосса должны быть в окрестности 0.0007-0.001. Ничего страшного, что лосс не падает, для CM это нормально. В конце обучения лосс может доходить до 0.005-0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QoidXNBgZgkY",
   "metadata": {
    "id": "QoidXNBgZgkY"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, pipe, train_dataloader, optimizer, loss_fn, num_grad_accum=1):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        latents, prompt_embeds = prepare_batch(batch, pipe)\n",
    "\n",
    "        loss = loss_fn(latents, prompt_embeds, model, pipe.scheduler)\n",
    "\n",
    "        # Обновляем параметры\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.detach().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5f675",
   "metadata": {
    "id": "79a5f675"
   },
   "outputs": [],
   "source": [
    "num_grad_accum = 2 # обновляем параметры каждые 2 шага\n",
    "\n",
    "train_loop(cm_unet, pipe, train_dataloader, optimizer, ct_loss, num_grad_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808dc13",
   "metadata": {
    "id": "8808dc13"
   },
   "source": [
    "## Задание 5\n",
    "\n",
    "### Генерация с помощью обученной консистенси модели\n",
    "\n",
    "Настало время погенерировать картинки с помощью нашей модели.\n",
    "Напомним, что мы не можем для консистенси моделей использовать DDIM и другие классические солверы для диффузии.\n",
    "Нам нужен специальный сэмплер для CM, который схематично изображен на картинке ниже:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://i.postimg.cc/66bWLvnh/cd-sampling.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "Чуть более формально:\n",
    "\n",
    "$x_{t_n} \\sim {N}(0, I)$\n",
    "\n",
    "$for\\ t_i \\in [t_n, ..., t_1]:$\n",
    "\n",
    "* $\\epsilon \\leftarrow unet(x_{t_i})$\n",
    "\n",
    "* $x_0 \\leftarrow DDIM(\\epsilon, x_{t_i}, t_i, 0)$\n",
    "\n",
    "* $x_{t_{i-1}} \\leftarrow q(x_{t_{i-1}} | x_0)$\n",
    "\n",
    "\n",
    "**Classifier-free guidance (CFG)**\n",
    "\n",
    "Также вам надо реализовать поддержку CFG в CM сэмплирование. Вспомним формулу:\n",
    "\n",
    "$\\epsilon_w = {\\color{blue}{\\epsilon_{uncond}}} + w \\cdot (\\epsilon_{cond} - \\epsilon_{uncond})$, где $w \\geq 1$\n",
    "\n",
    "**Обратим внимание**, что режим \"без гайденса\" соотвествует $w = 1$, что немного контринтуитивно, но в большинстве реализаций будет встречаться именно такой вид этой формулы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c47c2",
   "metadata": {
    "id": "6f1c47c2"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def consistency_sampling(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    num_inference_steps=4,\n",
    "    generator=None,\n",
    "    num_images_per_prompt=4,\n",
    "    guidance_scale=1\n",
    "):\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "    device = pipe._execution_device\n",
    "\n",
    "    # Извлекаем эмбеды из текстовых промптов. Реализуйте вызов pipe.encode_prompt\n",
    "    do_classifier_free_guidance = guidance_scale > 0\n",
    "    prompt_embeds, null_prompt_embeds = <YOUR CODE HERE>\n",
    "    assert prompt_embeds.dtype == null_prompt_embeds.dtype == torch.float16\n",
    "\n",
    "    # Настраиваем параметры scheduler-a\n",
    "    assert pipe.scheduler.config['timestep_spacing'] == 'trailing'\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Создаем батч латентов из N(0,I)\n",
    "    latents = <YOUR CODE HERE>\n",
    "\n",
    "    for i, t in enumerate(tqdm(pipe.scheduler.timesteps)):\n",
    "        t = torch.tensor([t] * len(latents)).to(device)\n",
    "        zero_t = torch.tensor([0] * len(latents)).to(device)\n",
    "\n",
    "        cond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            uncond_noise_pred = <YOUR CODE HERE>\n",
    "            noise_pred = <YOUR CODE HERE>\n",
    "        else:\n",
    "            noise_pred = cond_noise_pred\n",
    "\n",
    "        # Получаем x_0 оценку из x_t\n",
    "        x_0 = <YOUR CODE HERE>\n",
    "\n",
    "        if i + 1 < num_inference_steps:\n",
    "            # Переход на следующий шаг\n",
    "            s = pipe.scheduler.timesteps[i+1]\n",
    "            s = torch.tensor([s] * len(latents)).to(device)\n",
    "\n",
    "            latents = <YOUR CODE HERE>\n",
    "        else:\n",
    "            # Последний шаг\n",
    "            latents = x_0\n",
    "\n",
    "        latents = latents.half()\n",
    "\n",
    "    image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "    do_denormalize = [True] * image.shape[0]\n",
    "    image = pipe.image_processor.postprocess(image, output_type=\"pil\", do_denormalize=do_denormalize)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57927112",
   "metadata": {
    "id": "57927112"
   },
   "source": [
    "Попробуем сгененировать что-то нашей моделью. Можно поиграться с разными сидами и гайденс скейлами.\n",
    "\n",
    "Референс, что примерно должно получиться на этом этапе для guidance_scale=2. Как видите, картинки стали почетче, но пока все еще так себе.\n",
    "\n",
    "![img](https://i.postimg.cc/2jctrptY/ct-images.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2dc5a8",
   "metadata": {
    "id": "cf2dc5a8"
   },
   "outputs": [],
   "source": [
    "pipe.unet = cm_unet.eval().to(torch.float16)\n",
    "assert cm_unet.active_adapter == 'ct'\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "guidance_scale = 3\n",
    "\n",
    "# Заменяем генерацию пайплайном на наше сэмплирование.\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11670976",
   "metadata": {
    "id": "11670976"
   },
   "source": [
    "# Consistency Distillation\n",
    "\n",
    "## Задание №6\n",
    "\n",
    "\n",
    "Теперь давайте попробуем перейти к постановке дистилляции, где шаг из $x_t$ в $x_s$ будет делаться не аналитически, а c помощью модели учителя.  \n",
    "\n",
    "$\\mathbf{x}_t = q(\\mathbf{x}_t | \\mathbf{x}_0)$\n",
    "\n",
    "$\\mathbf{x}_s = DDIM(\\epsilon_\\theta(\\mathbf{x}_t, t), \\mathbf{x}_t, t, s)$\n",
    "\n",
    "**Замечание:**\n",
    "В text-to-image генерации *classifier-free guidance (CFG)* играет очень важную роль для получения хорошего качества с помощью диффузии.\n",
    "CFG меняет траектории ODE и раз нам он важен, то давайте и дистиллировать траектории с CFG.\n",
    "\n",
    "Поэтому для получения точки $\\mathbf{x}_{s}$ мы будем использовать шаг учителя с CFG. Это важное отличие от CT сеттинга - там мы не можем моделировать гайденс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98399cfb",
   "metadata": {
    "id": "98399cfb"
   },
   "outputs": [],
   "source": [
    "unet = unet.to(torch.float32)\n",
    "unet.train()\n",
    "assert unet.dtype == torch.float32\n",
    "\n",
    "# Добавляем новые LoRA адаптеры для CD модели\n",
    "cm_unet.add_adapter(\"cd\", lora_config)\n",
    "cm_unet.set_adapter(\"cd\")\n",
    "\n",
    "# Пересоздаем оптимизатор\n",
    "optimizer = torch.optim.AdamW(cm_unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eda898",
   "metadata": {
    "id": "67eda898"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_xs_from_xt_with_teacher(\n",
    "    x_0, x_t, t, s, # Не все эти аргументы могут быть вам нужны\n",
    "    scheduler,\n",
    "    prompt_embeds,\n",
    "    teacher_unet,\n",
    "    guidance_scale,\n",
    "    **kwargs\n",
    "):\n",
    "    # Делаем предсказание учителем в кондишион случае: подаем эмбеды текста\n",
    "    cond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Для CFG нам нужно делать предсказания в unconditional случае.\n",
    "    # Для T2I моделей, мы будем это моделировать предсказаниями для пустого промпта \"\"\n",
    "    # Извлечем эмбеды из пустого промпта и размножить их до размера батча\n",
    "    uncond_input_ids = pipe.tokenizer(\n",
    "        [\"\"], return_tensors=\"pt\", padding=\"max_length\", max_length=77\n",
    "    ).input_ids.to(\"cuda\")\n",
    "\n",
    "    uncond_prompt_embeds = pipe.text_encoder(uncond_input_ids)[0].expand(\n",
    "        *prompt_embeds.shape\n",
    "    )\n",
    "\n",
    "    # Затем прогоняем модель для пустых промптов\n",
    "    uncond_noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Применяем CFG формулу и получаем итоговый предикт учителя\n",
    "    noise_pred = <YOUR CODE HERE>\n",
    "\n",
    "    # Получаем x_s из x_t\n",
    "    x_s = <YOUR CODE HERE>\n",
    "    return x_s\n",
    "\n",
    "\n",
    "# Сразу зададим внутрь модель учителя и guidance_scale\n",
    "get_xs_from_xt_with_teacher = functools.partial(\n",
    "    get_xs_from_xt_with_teacher,\n",
    "    teacher_unet=teacher_unet,\n",
    "    guidance_scale=7.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f39be",
   "metadata": {
    "id": "765f39be"
   },
   "source": [
    "Еще, как показано в работе [Improved Techniques for Training Consistency Models](https://arxiv.org/pdf/2310.14189).\n",
    "L2 лосс не самый оптимальный выбор для консистенси моделей.\n",
    "Давайте в CD обучении также заменим MSE лосс на pseudo-huber лосс из статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a5bac",
   "metadata": {
    "id": "878a5bac"
   },
   "outputs": [],
   "source": [
    "def pseudo_huber_loss(\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    c=0.001\n",
    "):\n",
    "    loss = <YOUR CODE HERE>\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12855c88",
   "metadata": {
    "id": "12855c88"
   },
   "outputs": [],
   "source": [
    "cd_loss = functools.partial(\n",
    "    cm_loss_template,\n",
    "\n",
    "    loss_fn=pseudo_huber_loss,\n",
    "    get_boundary_timesteps=get_zero_boundary_timesteps,\n",
    "    get_xs_from_xt=get_xs_from_xt_with_teacher\n",
    ")\n",
    "\n",
    "assert cm_unet.active_adapter == 'cd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db78ad",
   "metadata": {
    "id": "79db78ad"
   },
   "source": [
    "**Теперь обучим модель в CD режиме**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3035cf7",
   "metadata": {
    "id": "a3035cf7"
   },
   "outputs": [],
   "source": [
    "num_grad_accum = 2 # обновляем параметры каждые 2 шага\n",
    "\n",
    "train_loop(cm_unet, pipe, train_dataloader, optimizer, cd_loss, num_grad_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971622aa",
   "metadata": {
    "id": "971622aa"
   },
   "source": [
    "### Снова сэмплируем\n",
    "\n",
    "Обратим внимание, что тут мы сэмпилруем без гайденса, потому что мы его уже частично прокинули в модель, когда делали шаг учителя с CFG.\n",
    "\n",
    "Снова для референса приводим картинки на этом этапе:\n",
    "\n",
    "![img](https://i.postimg.cc/mgrsxVfW/download-1.png)\n",
    "\n",
    "**Ваши картинки не обязаны совпадать: у вас могут быть немного менее/более качественные. Небольшая разница по качеству на оценку не влиет.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e13eb6",
   "metadata": {
    "id": "46e13eb6"
   },
   "outputs": [],
   "source": [
    "# Подставляем нашу новую обученную модель в пайплайн\n",
    "pipe.unet = cm_unet.eval().to(torch.float16)\n",
    "assert cm_unet.active_adapter == 'cd'\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "guidance_scale = 0\n",
    "\n",
    "images = <YOUR CODE HERE>\n",
    "\n",
    "visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be7c1e",
   "metadata": {
    "id": "30be7c1e"
   },
   "source": [
    "#### Давайте посмотрим на картинки для других промптов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae76af",
   "metadata": {
    "id": "4bae76af"
   },
   "outputs": [],
   "source": [
    "validation_prompts = [\n",
    "    \"A sad puppy with large eyes\",\n",
    "    \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n",
    "    \"A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece\",\n",
    "    \"A girl with pale blue hair and a cami tank top\",\n",
    "    \"A lighthouse in a giant wave, origami style\",\n",
    "    \"belle epoque, christmas, red house in the forest, photo realistic, 8k\",\n",
    "    \"A small cactus with a happy face in the Sahara desert\",\n",
    "    \"Green commercial building with refrigerator and refrigeration units outside\",\n",
    "]\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "\n",
    "    images = <YOUR CODE HERE>\n",
    "\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294acc5",
   "metadata": {
    "id": "9294acc5"
   },
   "source": [
    "# Multi-boundary Сonsistency Distillation\n",
    "<div>\n",
    "<img src=https://i.postimg.cc/tTzCv476/multi-cd.jpg width=600>\n",
    "<div>\n",
    "\n",
    "   В конце мы рассмотрим недавнюю модификацию CD, *Multi-boundary CD*, где интегрируем не всю траекторию сразу и потом сэмплируем с возвращением назад, а разбиваем траектории на $K$ отрезков и применяет CD внутри каждого отрезка независимо. Например, на картинке выше у нас два отрезка: зеленым и красным выделены две граничные точки. \n",
    "Для классического CD, рассмотренного ранее, у нас только одна граничная точка в $t = 0$ \n",
    "    \n",
    "**Обратим внимание**, что сэмплирование становится детерминистичным и можно снова использовать DDIM солвер, где число шагов равно числу интервалов $K$, на которые мы разбили траектории во время обучения.\n",
    "\n",
    "Этот метод гораздо лучше работает чем обычный CD, потому что решать задачу CD на отрезках, а не на всей траектории, гораздо проще. В текущем задании мы разобьем траекторию на $K=4$ отрезка.\n",
    "\n",
    "Подробнее почитать можно в этой [статье](https://arxiv.org/pdf/2403.06807)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f05839",
   "metadata": {
    "id": "95f05839"
   },
   "source": [
    "## Задание №7 (0.25 балла, сдается в контесте)\n",
    "\n",
    "Ниже реализуйте функцию, которая для $K=4$ отрезков будет сопоставлять таймстепам соответствующие граничные точки.\n",
    "\n",
    "Например, для $K=2$ отрезков граничные точки будут: [0, 499]\n",
    "\n",
    "$0 \\leq t < 499$ -> граничная точка - $0$\n",
    "\n",
    "$499 \\leq t < 999$ -> граничная точка - $499$\n",
    "\n",
    "**Замечание:** помним, что интервал между $t$ и $s$ - 20 шагов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2dbb2",
   "metadata": {
    "id": "c7d2dbb2"
   },
   "outputs": [],
   "source": [
    "def get_multi_boundary_timesteps(\n",
    "    timesteps,\n",
    "    num_boundaries=4,\n",
    "    num_timesteps=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Для батча таймстепов определяем соответствующие граничные точки.\n",
    "    params:\n",
    "        timesteps: torch.Tensor(batch_size, device='cuda')\n",
    "    returns:\n",
    "        boundary_timesteps: torch.Tensor(batch_size, device='cuda')\n",
    "    \"\"\"\n",
    "    # Здесь важно повыводить timesteps и boundary_timesteps перед обучением, \n",
    "    # чтобы не перелетать граничные точки и при этом иногда попадать в них.\n",
    "  \n",
    "    <YOUR CODE HERE>\n",
    "    return boundary_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1c0eb",
   "metadata": {
    "id": "c4c1c0eb"
   },
   "outputs": [],
   "source": [
    "multi_cd_loss = functools.partial(\n",
    "    cm_loss_template,\n",
    "\n",
    "    loss_fn=pseudo_huber_loss,\n",
    "    get_boundary_timesteps=get_multi_boundary_timesteps,\n",
    "    get_xs_from_xt=get_xs_from_xt_with_teacher\n",
    ")\n",
    "assert cm_unet.active_adapter == 'multi-cd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86175a",
   "metadata": {
    "id": "3d86175a"
   },
   "source": [
    "**Теперь обучим  Multi-boundary CD модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded4086",
   "metadata": {
    "id": "aded4086"
   },
   "outputs": [],
   "source": [
    "num_grad_accum = 2 # обновляем параметры каждые 2 шага\n",
    "\n",
    "train_loop(cm_unet, pipe, train_dataloader, optimizer, multi_cd_loss, num_grad_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28f872",
   "metadata": {
    "id": "2e28f872"
   },
   "source": [
    "### И в последний раз сэмплируем\n",
    "\n",
    "**Важно:** теперь у нас появляется возможно сэмплировать детерминистично с помощью оригинального солвера DDIM за 4 шага. Так что возвращаем сэмплирование исходным pipe-ом.\n",
    "\n",
    "Ниже прикрепляем референс и напомним, что у вас картинки могут отличаться и быть чуть хуже/лучше.\n",
    "![img](https://i.postimg.cc/gjYzQ0n2/download-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa9bcc",
   "metadata": {
    "id": "90aa9bcc"
   },
   "outputs": [],
   "source": [
    "pipe.unet = cm_unet.eval().to(torch.float16)\n",
    "assert cm_unet.active_adapter == 'multi-cd'\n",
    "\n",
    "guidance_scale = 1\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "\n",
    "    images = <YOUR CODE HERE>\n",
    "\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e782d",
   "metadata": {
    "id": "403e782d"
   },
   "source": [
    "## Задание №8\n",
    "\n",
    "Все, что осталось сделать - это загрузить ваши обученные модельки на huggingface_hub.\n",
    "Это очень популярный и удобный способ для хранения моделей, которые легко можно загружать и подставлять в модель.\n",
    "Другими словами GitHub для моделей и датасетов.\n",
    "\n",
    "1) Создайте аккаунт на [huggingface.co](huggingface.co)\n",
    "\n",
    "2) Получите свой HF токен, который можно получить здесь: https://huggingface.co/settings/tokens\n",
    "\n",
    "3) Создайте репозиторий для ваших моделями https://huggingface.co/new\n",
    "\n",
    "**Важно: перед отправкой нотбука на проверку, не забудьте удалить свой HF токен!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e08bf",
   "metadata": {
    "id": "7c8e08bf"
   },
   "outputs": [],
   "source": [
    "cm_unet.push_to_hub(\n",
    "    <YOUR REPO NAME HERE>, # \"<username>/<repo-name>\"\n",
    "    token=<YOUR HF TOKEN HERE>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214be7df",
   "metadata": {
    "id": "214be7df"
   },
   "source": [
    "Пример, как должен выглядеть результат выполнения команды: https://huggingface.co/dbaranchuk/cv-week-final-task-example\n",
    "\n",
    "Давайте проверим, что загрузка модели корректно работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9caee95",
   "metadata": {
    "id": "b9caee95"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_cm_unet = PeftModel.from_pretrained(\n",
    "    unet,\n",
    "    <YOUR REPO NAME HERE>,\n",
    "    token=<YOUR HF TOKEN HERE>,\n",
    "    subfolder='multi-cd',\n",
    "    adapter_name=\"multi-cd\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad678713",
   "metadata": {
    "id": "ad678713"
   },
   "outputs": [],
   "source": [
    "pipe.unet = loaded_cm_unet.eval().to(torch.float16)\n",
    "assert loaded_cm_unet.active_adapter == 'multi-cd'\n",
    "\n",
    "guidance_scale = 1\n",
    "\n",
    "for prompt in validation_prompts:\n",
    "    generator = torch.Generator(device=\"cuda\").manual_seed(1)\n",
    "\n",
    "    images = <YOUR CODE HERE>\n",
    "кст\n",
    "    visualize_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508112b9",
   "metadata": {
    "id": "508112b9"
   },
   "source": [
    "**На этом все! Ура!**\n",
    "\n",
    "<div>\n",
    "<img src=\"https://i.postimg.cc/jq8F6Yvk/photo-2024-11-27-21-57-13.jpg\" width=400>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582f3d8",
   "metadata": {
    "id": "dh7XtMVcUVDm"
   },
   "source": [
    "### P.S. Некоторые примеры плохих генераций, которые могут возникать при выполнении задания\n",
    "\n",
    "#### Неправильный сэмплинг\n",
    "\n",
    "![img](https://i.postimg.cc/TYM2vR4M/photo-2024-11-29-02-14-50.jpg)\n",
    "\n",
    "![img](https://i.postimg.cc/XYnVXsWs/photo-2024-11-29-02-16-00.jpg)\n",
    "\n",
    "#### Ошибки в обучении\n",
    "\n",
    "![img](https://i.postimg.cc/XJfHZk4g/photo-2024-11-29-02-35-05.jpg)\n",
    "![img](https://i.postimg.cc/RF5pVD5F/photo-2024-11-29-02-24-38.jpg)\n",
    "\n",
    "#### Необученная модель\n",
    "\n",
    "![img](https://i.postimg.cc/8zwTvLPN/photo-2024-11-29-01-58-36.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b6b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
